## Rebuttal

We sincerely thank all the reviewers for their detailed and constructive feedback. Your insights and suggestions have helped us identify areas for improvement and clarify the presentation of our work. We deeply appreciate your recognition of the novelty and potential impact of our approach, as well as the thoughtful questions and concerns raised during your evaluations. In this response, we categorize the feedback into two sections for clarity:
Weaknesses (W): Concerns or areas for improvement raised in the reviews.
Questions (Q): Specific queries or clarifications sought regarding the methodology, results, or framework.
We aim to address all points comprehensively, providing additional context, experimental evidence where possible, and detailing revisions planned for the manuscript to address your feedback effectively.

The following animation can help better understand RLPO.

<p align="center">
  <img src="Images/RLPO Post.gif" alt="animation" width="50%">
  <p style="width: 50%; margin: 0 auto;">Our proposed algorithm, RLPO, iteratively refines the concepts c<sub>i</sub> that can be generated by a Stable Diffusion (SD) model by optimizing SD weights based on an action a<sub>i</sub>. Each step in this update process provides an explanation at a different level of abstraction.</p>
</p>
