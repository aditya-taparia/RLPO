Thank you for identifying the novelty and appreciating the experiments. We hope the following explanations will clarify the queries for the reviewer.

### Q1: It seems that the users need to have a good understanding of the target concept in order to generate good concept seeds (i.e, proper VQA design and possibly selective prompts). Is it possible to extract concepts from a large paragraph of description texts without prior human knowledge?

We agree with the reviewer’s observation that in our current setup, in order to generate good concept seeds proper VQA design and selective prompts are needed. But because of the modularity of our approach, the generation of seed prompts can be replaced by any other concept generation method. As described in [1], the end user can extract concepts from text descriptions without prior human knowledge and directly use it in our proposed method.

[1] Zang, Yuan, et al. "Pre-trained vision-language models learn discoverable visual concepts." arXiv preprint arXiv:2404.12652 (2024).

### Q2: After the concept pictures are generated by SD+LORA, it is unsure how you can align them with the original input picture?

We employ CLIPSeg, a transformer-based segmentation model, to establish visual correspondence between generated concepts and regions in the input images (See Section 4.4 Figure 5). By feeding the generated concept images as prompts into CLIPSeg, we produce heat maps highlighting areas in the input image that resemble the generated concept. This allows us to localize abstract concepts (e.g., “stripes”, “mud”) within the original images, enabling interpretable alignment between concept space and class-specific features.

We hope these responses clarify your concerns. We appreciate your recognition of the work's broader implications and suggestions for future extensions, which we plan to incorporate.