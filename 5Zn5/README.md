We thank the reviewer for the constructive comments and identifying the method as interesting. We have addressed the concerns in weakness and questions as follows.

### Weakness

To summarize:

**Our claim:** When considering automated concept set creation techniques (that are retrieval based), the proposed generative method can come up with novel concepts that trigger the neural network (unveiling such new patterns can help engineers fix models).

**Experimental evidence to support the claim:**
1. Quantitative: Table 4 shows that our method can generate new concepts.
2. Qualitative: Figure 4 illustrates some results of Table 4.
3. Human evaluation: Most humans cannot imagine certain patterns will even trigger the neural network.

Let us elaborate on this.

To evaluate the concept's relevance to the model, we evaluate the method’s success through multiple metrics, as reflected in the experiments section. Table 4 provides quantitative evidence for the relevance of the generated concepts by measuring TCAV scores. Higher TCAV scores indicate that the concepts discovered are aligned with the model’s internal representations, confirming their significance for decision-making. This shows that the generated concepts "matter" to the model. We use metrics such as cosine similarity and euclidean distance to show that the generated concepts are novel—generated concepts are farther away from the class images, indicating that concepts generated by our method are not a subset of class data as in retrieval methods.

We also conducted a human experiment (results shown in Table 2), where when asked for identifying relevant concepts within generated, retrieved or both, most volunteers irrespective of laymen or experts mostly picked retrieved concepts, even though both were equally important to the network. This experiment indicates that it is not easy for humans to imagine concepts by themselves as the neural network’s learning process is different to ours.

Furthermore, to help humans understand what each relevant concept represents, we made use of ClipSeg to identify the intersection between generated concept images and test images. Figure 6 highlights the regions each relevant concept represents in the test image.

---

### Question 1

For the experiments presented in the paper, the RLPO framework with DQN+Diffusion typically requires approximately 8 hours per class on a machine equipped with an NVIDIA RTX 4090 GPU to train, with the most computationally intensive step being the iterative fine-tuning of the generative model. This concept set creation is a one-time investment for an application of interest. Evaluating TCAV is pretty much real-time once we have the concept set. We will include this discussion in the revised version of the paper.

### Question 2

The images in Figure 6 are final outputs of the RLPO framework–this is what most people want as explanations. They are the lowest level of abstraction in explanations. However, if someone wants higher levels of abstractions in explanations, as shown in Figure 5, they can also obtain them. In Figure 5, the explanation of the tiger class progresses through levels of abstraction from high to low: the importance of zoos, followed by animals in zoos, then striped animals in zoos, and finally orange-and-black striped animals, and so on.
