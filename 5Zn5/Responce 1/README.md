We thank the reviewer for their prompt response and giving us the opportunity to clarify the queries.

We believe the confusion happened because “human” was used as an overloaded term. In the classical TCAV setting, two groups of humans are involved: those who create concept sets offline (“creator humans”) and those who utilize these concepts online (“user humans”). When we state “humans cannot,” we are specifically referring to the creator humans, as our contribution is developing an algorithm for concept set creation/generation rather than the downstream application of an existing method. To clarify, we do not claim that the generated concepts are not understandable to the user humans. **Rather, we argue that the concepts that the model indeed uses can be divided into two groups: concepts that creator humans can think of and those they cannot (i.e., novel concepts). Our method generates explanations from both groups, and both types of concepts are understandable to user humans.**

Understandable should be corrected as guessed. For instance, in the reviewer’s summary “concepts are not likely to be ~~understood~~ guessed by creator humans.” Vanilla TCAV requires creator humans to come up with (i.e., guess) a set of concepts to test ahead of time. In other words, it assumes the creator humans know a set of explanations ahead of time as TCAV only picks from the set which concept/explanation is correct. When we tried to apply TCAV in some real-world applications, we found out that it is impossible for creator humans to guess all such concepts/explanations ahead of time. That motivated us to generate concepts.

**Interpretation of results in Table 2:**
Hypothesis: Creator humans struggle to guess all important concepts that truly matter to the network.

Experimental setup: Rather than asking creator humans to come up with large concept sets, we present them with two concepts and ask them which one they would include in the concept set. To this end, as detailed in Appendix D6 (and the following image), we show creator humans a test image along with two explanations—one obtained from a retrieval-based method and the other generated by our method (without revealing the experimental setup to them). Those creator humans were asked to determine which of the two explanations could have influenced the network's decision that the test image, for example, represents a zebra. They could choose the first explanation, the second, or both.

Metrics: All generated concepts matter to the network as they have a high TCAV score. Out of them, how many of them are identifiable by creator humans?

Results: The results in Table 2 shows that creator humans often recognized explanations from retrieval-based methods, as these align with cropped elements of the test images. However, they were less likely to guess generated explanations, even though these are valid concepts that influence the neural network's decision and have high TCAV scores. This confirms that our method successfully discovers valid explanations that are not immediately apparent to creator humans.

<p align="center">
  <img src="../Images/R5_1.jpg" alt="Image 1" width="40%">
</p>

**Do concepts explain the model?**
We believe the philosophical question of whether the concepts really explain the model is valid for all concept-based techniques. If a human collects a concept set, is it guaranteed that another human will perceive the same pattern? Not necessarily, as cognitive biases can influence interpretation. If a retrieval method crops and collects some parts of images, is it always understandable to humans? We believe the same argument is true for generated concepts as well.
The concepts our method generates are explainable by design since RLPO iteratively fine-tunes the diffusion model to generate images with high TCAV scores. Additionally, to further verify whether these generated concepts are indeed explaining the neural network we performed the classical c-deletion experiment. As shown in Fig. 8, we see that gradually removing these concepts from the input leads to drop in average-class accuracy. If they were some irrelevant concepts, we would not have seen such a drop.

The user human would see something similar to Figure 6 (see below). For a zebra test point, the user will see three sets of concept images shown on the right, with the most important at the top. The red set (highest TCAV) highlights stripes, and on the left, it shows which parts of the test image these concepts are most relevant to. The other two sets provide supplementary explanations. The green set illustrates a green wooded background (note that Stable Diffusion’s seed for this was running, but RLPO fine-tuned it to generate such backgrounds—seeds are used only for our analysis and are not important for the user human). The blue concept set depicts a brown background with some green on the horizon. These supplementary explanations primarily describe the background. Consolidating everything, while the most important concept for the network to classify a zebra is black-and-white stripes, its habitat, including a brown background and green trees, also contributes to the classification.

<p align="center">
  <img src="../Images/R5_2.jpg" alt="Image 1" width="40%">
</p>

“To this end, we leverage the state-of-the-art text-to-image generative models to generate high quality explainable concepts.” What we meant by high quality is the quality of images generated by the model. We can rephrase and tone it down. We acknowledge that our perspective was rooted in the creator human’s mindset as we were on a quest to automate the concept set creation process. As highlighted in the concluding experiments of section 4.5, we truly hope our work would benefit 1) engineers debug issues in neural networks (Figure 9) and 2) make the use of TCAV easier in downstream applications (Figure 10).
